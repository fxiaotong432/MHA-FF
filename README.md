# MHA-FF
Code for the article 'MHA-FF: A Multi-Head Attention Model for Adaptive Fusion of Heterogeneous Features in Lung Nodule Classification'.
## Abstract
In medical scenarios, image classification using heterogeneous feature poses a significant challenge. This is especially pertinent in lung cancer diagnosis, where accurately distinguishing between pre-invasive lung adenocarcinoma (Pre-IAs) and invasive adenocarcinoma (IAs), as well as classifying the subtypes of IAs, is crucial. Traditional methods have focused on manually extracted radiomics characteristics, whereas recent advancements in deep learning have enabled the extraction of more intricate, high-level features. Effectively merging the information from both methods to improve lung cancer diagnosis represents a problem of great interest. In this study, we proposed a multi-head attentional feature fusion (MHA-FF) model for not only distinguishing IAs from Pre-IAs, but also for distinguishing the different subtypes of IAs. To enhance accurate prediction, we utilized a combination of deep learning and radiomics features, each extracted via distince methodologies from computed tomography images. Specifically, we integrated these heterogeneous features using an adaptive fusion module that can learn attention-based discriminative features. The effectiveness of our proposed method is demonstrated using real-world data collected from a multi-center cohort.
## Method
The input is an array of CT images with regard to a specific patient, with nodule centers manually labelled, in advance. The input is first passed through two modules (A and B), in parallel. Module A involves knowledge-driven radiomics feature extraction, while Module B involves data-driven deep feature extraction. For module A, we obtained both the lung mask and multi-scale nodule mask to extract various radiomics features. A feature-selection mechanism (i.e., a sure independence screening (SIS) feature-selection mechanism) is designed to reveal the radiomics features with a significant impact on the prediction, i.e., $x^r$. For module B, a cropped image $I_{N_{k}}$ at the nodule center is fed into a 2D-convoluted neural network (CNN) to extract a deep semantic feature $x^d$ for each slice. Next, both the radiomics and deep features are fed into a multi-head attentional block (i.e., module C) for feature fusion. The final probabilistic prediction of LUAD subtype (i.e., HDA, MDA, and PDA) is obtained by mean pooling plus softmax activation. It should be noted that the output will be changed to a binary indicator when the task becomes distinguishing IAs from Pre-IAs. 

